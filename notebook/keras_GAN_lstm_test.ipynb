{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-25T23:53:38.127284Z",
     "start_time": "2019-03-25T23:53:38.104244Z"
    }
   },
   "outputs": [],
   "source": [
    "import os, sys, io\n",
    "import joblib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm_notebook, tnrange\n",
    "\n",
    "from keras.layers import (Input, Dense, LSTM, Dropout, TimeDistributed,\n",
    "                          AveragePooling1D, Flatten, InputLayer, BatchNormalization)\n",
    "from keras.layers.core import Reshape\n",
    "from keras.models import Model, Sequential\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras import initializers\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn import model_selection\n",
    "\n",
    "sys.path.append('%s/lib' % (os.path.abspath('..')))\n",
    "from SyntheticLCGenerator import synthetic_light_curve_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-25T23:53:38.838845Z",
     "start_time": "2019-03-25T23:53:38.825077Z"
    }
   },
   "outputs": [],
   "source": [
    "# Let Keras know that we are using tensorflow as our backend engine\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "\n",
    "# Main Path\n",
    "main_path = os.path.abspath('..')\n",
    "\n",
    "# To make sure that we can reproduce the experiment and get the same results\n",
    "np.random.seed(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-25T23:53:39.507676Z",
     "start_time": "2019-03-25T23:53:39.313435Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_synthetic_time_series(data_path=None, n_samples=14000, seq_length=50,\n",
    "                               time_span=4, n_bands=1, n_signals=1, SNR_min=5,\n",
    "                               f0_interval='narrow',\n",
    "                               use_time=True, use_err=True):\n",
    "\n",
    "    if f0_interval == 'narrow':\n",
    "        f0_inter = [1/10., 1/1.]\n",
    "    elif f0_interval == 'wide':\n",
    "        f0_inter = [1/100., 1/0.01]\n",
    "    else:\n",
    "        print('Plese define frequency interval...')\n",
    "        return\n",
    "\n",
    "    if os.path.exists(data_path):\n",
    "        print('Loading from: ', data_path)\n",
    "        aux = np.load(data_path).item()\n",
    "        samples = aux['samples']\n",
    "        labels = np.array(aux['labels'])[:, None]\n",
    "        periods = aux['periods']\n",
    "        if n_bands == 1:\n",
    "            samples = samples.reshape(samples.shape[0], samples.shape[2],\n",
    "                                      samples.shape[3])\n",
    "        del aux\n",
    "        if use_time and not use_err:\n",
    "            samples = samples[:, :, 0:2]\n",
    "        if not use_time and not use_err:\n",
    "            samples = samples[:, :, 1:2]\n",
    "    return samples, labels, periods\n",
    "\n",
    "\n",
    "def load_real_time_series(data_path=None, survey='EROS2', n_bands=2,\n",
    "                          use_time=True, use_err=True):\n",
    "\n",
    "    if os.path.exists(data_path):\n",
    "        print('Loading from: ', data_path)\n",
    "        aux = joblib.load(data_path)\n",
    "        lcs = aux['lcs']\n",
    "        meta = aux['meta']\n",
    "        del aux\n",
    "        lcs = np.stack([x.values for x in lcs])\n",
    "        print(lcs.shape)\n",
    "        if n_bands == 1:\n",
    "            lcs = lcs[:, :, 0:3]\n",
    "        if use_time and not use_err:\n",
    "            lcs = lcs[:, :, 0:2]\n",
    "        if not use_time and not use_err:\n",
    "            lcs = lcs[:, :, 1:2]\n",
    "    return lcs, meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-25T23:53:40.470459Z",
     "start_time": "2019-03-25T23:53:40.367147Z"
    }
   },
   "outputs": [],
   "source": [
    "# You will use the Adam optimizer\n",
    "def get_optimizer(learning_rate):\n",
    "    return Adam(lr=learning_rate)\n",
    "\n",
    "\n",
    "def get_generator(optimizer, G_hidden_units, latent_dim, \n",
    "                  out_lc_len, n_feat, dropout=0.1):\n",
    "    generator = Sequential()\n",
    "\n",
    "    # LSRM ouputs last state\n",
    "    # generator.add(LSTM(G_hidden_units, input_shape=(out_lc_len, latent_dim),\n",
    "    #                    return_sequences=False))\n",
    "    # generator.add(Dense(out_lc_len*n_feat, activation='tanh'))\n",
    "    # generator.add(Reshape((out_lc_len, n_feat)))\n",
    "    \n",
    "    # using all LSTM outputs (sequence)\n",
    "    generator.add(LSTM(G_hidden_units, input_shape=(out_lc_len, latent_dim),\n",
    "                       return_sequences=True))\n",
    "    generator.add(Dropout(dropout))\n",
    "    \n",
    "    # generator.add(LSTM(G_hidden_units,\n",
    "    #                    return_sequences=True))\n",
    "    # generator.add(Dropout(dropout))\n",
    "\n",
    "    generator.add(TimeDistributed(Dense(n_feat, activation='tanh')))\n",
    "    \n",
    "    # fully connected layers\n",
    "    # generatortor.add(Dense())\n",
    "\n",
    "    generator.compile(loss='mean_squared_error', optimizer=optimizer)\n",
    "    return generator\n",
    "\n",
    "\n",
    "def get_discriminator(optimizer, D_hidden_units, lc_len,\n",
    "                      n_feat, dropout=0.1, D_output=1):\n",
    "    discriminator = Sequential()\n",
    "\n",
    "    # LSTM output last state\n",
    "    discriminator.add(LSTM(D_hidden_units, input_shape=(lc_len, n_feat),\n",
    "                           return_sequences=False))\n",
    "    # discriminator.add(Dense(D_hidden_units//2, activation='tanh'))\n",
    "    discriminator.add(Dropout(dropout))\n",
    "    discriminator.add(Dense(D_output, activation='softmax'))\n",
    "    \n",
    "    \n",
    "    # using all LSTM states, full sequence\n",
    "    # discriminator.add(BatchNormalization(axis=-1, input_shape=(lc_len, n_feat)))\n",
    "    # discriminator.add(LSTM(D_hidden_units, input_shape=(lc_len, n_feat),\n",
    "    #                        return_sequences=True))\n",
    "    # discriminator.add(Dropout(dropout))\n",
    "    \n",
    "    # discriminator.add(LSTM(D_hidden_units,\n",
    "    #                        return_sequences=True,\n",
    "    #                        kernel_initializer=initializers.RandomNormal(stddev=0.02)))\n",
    "    # discriminator.add(Dropout(dropout))\n",
    "    \n",
    "    # discriminator.add(TimeDistributed(Dense(D_output, activation='softmax')))\n",
    "    # discriminator.add(AveragePooling1D(pool_size=lc_len, data_format='channels_last'))\n",
    "    # discriminator.add(Flatten())\n",
    "\n",
    "    discriminator.compile(loss='binary_crossentropy',\n",
    "                          optimizer=optimizer, metrics=['acc'])\n",
    "    return discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-18T19:54:29.864930Z",
     "start_time": "2019-03-18T19:54:29.850848Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_gan_network(discriminator, latent_dim, out_lc_len, generator, optimizer):\n",
    "\n",
    "    # We initially set trainable to False since we only want\n",
    "    # to train either the generator or discriminator at a time\n",
    "    discriminator.trainable = False\n",
    "\n",
    "    # gan input (noise) will be 100-dimensional vectors\n",
    "    gan_input = Input(shape=(out_lc_len, latent_dim))\n",
    "\n",
    "    # the output of the generator (an image)\n",
    "    x = generator(gan_input)\n",
    "\n",
    "    # get the output of the discriminator\n",
    "    # (probability if the image is real or not)\n",
    "    gan_output = discriminator(x)\n",
    "    gan = Model(inputs=gan_input, outputs=gan_output)\n",
    "    gan.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "\n",
    "    return gan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Extra functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-18T19:54:29.914620Z",
     "start_time": "2019-03-18T19:54:29.872783Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a wall of generated time series\n",
    "def plot_generated_time_series(epoch, generated_lc, test_lc=None, examples=8,\n",
    "                               dim=(2, 4), figsize=(16, 4)):\n",
    "\n",
    "    fig, ax = plt.subplots(nrows=dim[0], ncols=dim[1], figsize=figsize)\n",
    "    for i in range(dim[0]):\n",
    "        for j in range(dim[1]):\n",
    "            if use_time and use_err:\n",
    "                if j == 0 and test_lc is not None:\n",
    "                    ax[i,j].errorbar(test_lc[i, :, 0], test_lc[i, :, 1],\n",
    "                                     yerr=test_lc[i, :, 2], fmt='k.')\n",
    "                else:\n",
    "                    ax[i,j].errorbar(generated_lc[(j+i) + j, :, 0], generated_lc[(j+i) + j, :, 1],\n",
    "                                     yerr=generated_lc[(j+i) + j, :, 2], fmt='b.')\n",
    "            elif use_time and not use_err:\n",
    "                if j == 0 and test_lc is not None:\n",
    "                    ax[i,j].plot(test_lc[i, :, 0], test_lc[i, :, 1], 'k.')\n",
    "                else:\n",
    "                    ax[i,j].plot(generated_lc[(j+i) + j, :, 0], generated_lc[(j+i) + j, :, 1], 'b.')\n",
    "            elif not use_time and not use_err:\n",
    "                if j == 0 and test_lc is not None:\n",
    "                    ax[i,j].plot(test_lc[i, :], 'k.')\n",
    "                else:\n",
    "                    ax[i,j].plot(generated_lc[(j+i) + j, :], 'b.')\n",
    "    plt.tight_layout()\n",
    "    # plt.savefig('gan_generated_image_epoch_%d.png' % epoch)\n",
    "    buf = io.BytesIO()\n",
    "    plt.savefig(buf, format='png')\n",
    "    buf.seek(0)\n",
    "    plt.show()\n",
    "    return buf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-18T19:54:29.933191Z",
     "start_time": "2019-03-18T19:54:29.921607Z"
    }
   },
   "outputs": [],
   "source": [
    "def write_log_scalar(callback, names, logs, step):\n",
    "    for name, value in zip(names, logs):\n",
    "        summary = tf.Summary()\n",
    "        summary_value = summary.value.add()\n",
    "        summary_value.simple_value = value\n",
    "        summary_value.tag = name\n",
    "        callback.writer.add_summary(summary, step)\n",
    "        callback.writer.flush()\n",
    "        \n",
    "def write_log_plot(callback, buf, step):\n",
    "    image = tf.image.decode_png(buf.getvalue(), channels=4)\n",
    "    image = tf.expand_dims(image, 0)\n",
    "    summary = tf.summary.image(\"Generated_Image\", image,\n",
    "                               max_outputs=1)\n",
    "    callback.writer.add_summary(tf.keras.backend.eval(summary), step)\n",
    "    callback.writer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-18T19:54:29.982274Z",
     "start_time": "2019-03-18T19:54:29.937744Z"
    }
   },
   "outputs": [],
   "source": [
    "def normalize(data, norm_time=False, scale_to=[0, 1], n_feat=2):\n",
    "    normed = np.zeros_like(data)\n",
    "    for i, lc in enumerate(data):\n",
    "        normed[i, :, n_feat-1] = (lc[:, n_feat-1] - np.min(lc[:, n_feat-1])) / \\\n",
    "            (np.max(lc[:, n_feat-1]) - np.min(lc[:, n_feat-1]))\n",
    "        if scale_to != [0, 1]:\n",
    "            normed[i, :, n_feat-1] = (normed[i, :, n_feat-1] * (scale_to[1] - scale_to[0])) + scale_to[0]\n",
    "        if norm_time:\n",
    "            normed[i, :, n_feat-2] = (lc[:, n_feat-2] - np.min(lc[:, n_feat-2])) / \\\n",
    "                (np.max(lc[:, n_feat-2]) - np.min(lc[:, n_feat-2]))\n",
    "            if scale_to != [0, 1]:\n",
    "                normed[i, :, n_feat-2] = (normed[i, :, n_feat-2] * (scale_to[1] - scale_to[0])) + scale_to[0]\n",
    "        else:\n",
    "            normed[i, :, n_feat-2] = lc[:, n_feat-2]\n",
    "    return normed\n",
    "\n",
    "\n",
    "def standarize(data, stand_time=False):\n",
    "    standar = np.zeros_like(data)\n",
    "    for i, lc in enumerate(data):\n",
    "        standar[i, :, 1] = (lc[:, 1] - np.mean(lc[:, 1])) / np.std(lc[:, 1])\n",
    "        if stand_time:\n",
    "            standar[i, :, 0] = (lc[:, 0] - np.mean(lc[:, 0])) / np.std(lc[:, 0])\n",
    "        else:\n",
    "            standar[i, :, 0] = lc[:, 0]\n",
    "    return standar\n",
    "\n",
    "\n",
    "def get_noise(n_samples, n_timesteps=50, latent_dim=5, use_time=True):\n",
    "    latent = np.random.normal(size=[n_samples, n_timesteps, latent_dim])\n",
    "    if use_time:\n",
    "        latent[:, :, 0] = sample_time(n=n_timesteps)\n",
    "    return latent\n",
    "\n",
    "\n",
    "def sample_time(n=50, t_range=[0, 4], irregular=True):\n",
    "    time = np.linspace(t_range[0], t_range[1], num=n*10)\n",
    "    time = np.random.choice(time, size=n, replace=False)\n",
    "    return np.sort(time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tsting area"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-21T19:53:14.016250Z",
     "start_time": "2019-02-21T19:53:13.837007Z"
    },
    "scrolled": false
   },
   "source": [
    "test = standarize(samples[:2], stand_time=True)\n",
    "plt.plot(samples[0,:,0], samples[0,:,1], '.k')\n",
    "plt.plot(test[0,:,0], test[0,:,1], '.r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-05T20:00:12.256646Z",
     "start_time": "2019-02-05T20:00:11.911755Z"
    }
   },
   "source": [
    "test_d = Sequential()\n",
    "test_d.add(LSTM(10, input_shape=(50,3), return_sequences=True))\n",
    "test_d.add(Dense(1, activation='sigmoid'))\n",
    "test_d.add(AveragePooling1D(50))\n",
    "test_d.add(Reshape((1, )))\n",
    "test_d.compile(loss='mean_squared_error', optimizer='adam')\n",
    "print(test_d.summary())\n",
    "print('LSTM 1 output shape: ',test_d.predict(samples[0:10]).shape)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-05T20:00:12.640864Z",
     "start_time": "2019-02-05T20:00:12.386173Z"
    }
   },
   "source": [
    "test_g = Sequential()\n",
    "test_g.add(LSTM(5, input_shape=(100,3), return_sequences=True))\n",
    "test_g.add(TimeDistributed(Dense(3)))\n",
    "test_g.compile(loss='mean_squared_error', optimizer='adam')\n",
    "print(test_g.summary())\n",
    "print('LSTM 1 output shape: ',test_g.predict(test_noise).shape)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-05T23:39:11.836468Z",
     "start_time": "2019-02-05T23:39:11.210364Z"
    }
   },
   "source": [
    "print('Original lc: ', samples[0].mean(), samples[0].std())\n",
    "for k in range(1):\n",
    "    plt.plot(samples[k], '.')\n",
    "plt.show()\n",
    "\n",
    "# define model\n",
    "inputs1 = Input(shape=(50, 1))\n",
    "lstm1 = BatchNormalization(axis=-1)(inputs1)\n",
    "model = Model(inputs=inputs1, outputs=lstm1)\n",
    "norm = test_d.predict(samples[0])\n",
    "\n",
    "print('Batch Norm output shape: ', norm.mean(), norm.std())\n",
    "plt.plot(norm, '.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-20T22:16:58.074696Z",
     "start_time": "2019-03-20T22:16:58.037881Z"
    }
   },
   "outputs": [],
   "source": [
    "# data\n",
    "n_samples = 28000\n",
    "n_obs = 50\n",
    "n_bands = 1\n",
    "use_time = True\n",
    "use_err = True\n",
    "n_feat = 3\n",
    "if (use_time and not use_err) or (not use_time and use_err):\n",
    "    n_feat = 2\n",
    "elif not use_time and not use_err:\n",
    "    n_feat = 1\n",
    "data_name = 'SynSine_time%s_err%s' % (str(use_time)[0], str(use_err)[0])\n",
    "# data_name = 'EROS2_trim215_augmented'\n",
    "\n",
    "# GAN architecture\n",
    "model_name = 'LSTM_GAN'\n",
    "learning_rate = 0.5 # [0.001, 0.005, 0.01, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.]\n",
    "latent_dim = 5\n",
    "D_hidden_units = 4  # [2, 4, 8, 16, 32, 64, 128]\n",
    "G_hidden_units = 4\n",
    "D_output = 1\n",
    "G_dropout = 0.2\n",
    "D_dropout = 0.1\n",
    "batch_size = 28\n",
    "n_epochs = 200\n",
    "N_gen_feat = n_feat\n",
    "save_interval = 5\n",
    "viz_interval = 10\n",
    "batch_count = round(7000*.6 / batch_size)\n",
    "n_examples = 8\n",
    "gen_lc_len = n_obs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Retrieve data"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "data_path = '%s/data/real/EROS2_meta+lcs_sample_lc_trim_augmented.pkl' % (main_path)\n",
    "samples, meta = load_real_time_series(data_path, use_time=use_time, use_err=use_err)\n",
    "\n",
    "samples = normalize(samples[:7000], norm_time=False, scale_to=[-1, 1], n_feat=n_feat)\n",
    "meta = meta[:7000]\n",
    "\n",
    "print('Samples shape: ', samples.shape)\n",
    "\n",
    "[x_train, x_test, \n",
    " y_train, y_test] = model_selection.train_test_split(samples, meta, \n",
    "                                                     train_size=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-20T22:17:04.951725Z",
     "start_time": "2019-03-20T22:17:03.251716Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from:  /Users/jorgetil/Astro/TL-GANs/data/synthetic/sine_nsamples28000_seqlength50_nbands1_nsig1_timespan4_SNR40_f0narrow.npy\n",
      "Samples shape:  (7000, 50, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jorgetil/miniconda3/envs/keras/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2179: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "data_path = ('%s/data/synthetic/sine_nsamples%i_seqlength%i_nbands%i_nsig%i_timespan%i_SNR%i_f0%s.npy'\n",
    "             % (main_path, n_samples, n_obs, n_bands, 1, 4, 40, 'narrow'))\n",
    "samples, labels, periods = load_synthetic_time_series(data_path, use_time=use_time, use_err=use_err)\n",
    "\n",
    "samples = normalize(samples[:7000], norm_time=True, scale_to=[-1, 1], n_feat=n_feat)\n",
    "labels = labels[:7000]\n",
    "periods = periods[:7000]\n",
    "\n",
    "print('Samples shape: ', samples.shape)\n",
    "\n",
    "[x_train, x_test, \n",
    " y_train, y_test] = model_selection.train_test_split(samples, labels, \n",
    "                                                     train_size=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-20T22:17:05.699686Z",
     "start_time": "2019-03-20T22:17:05.634785Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.94507255, -0.48956634, -0.75522786, ..., -0.27313974,\n",
       "        -0.6523365 , -0.59474058],\n",
       "       [-0.83164189, -0.23011817, -0.47375412, ..., -0.94711297,\n",
       "        -0.8877978 , -0.41022852],\n",
       "       [-0.47524681, -0.50230085, -0.23489792, ..., -0.74311022,\n",
       "        -0.33752301, -0.52287157],\n",
       "       ...,\n",
       "       [-0.38914805, -0.38688117,  0.24508006, ...,  0.47683851,\n",
       "        -0.8564403 , -1.        ],\n",
       "       [-1.        ,  0.49936518, -0.488843  , ..., -0.54300996,\n",
       "        -0.53233034, -0.48740375],\n",
       "       [-0.3201309 , -0.74912054, -0.58705289, ..., -0.89226871,\n",
       "        -0.51868404, -0.49331167]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples[:,:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-20T22:17:22.859970Z",
     "start_time": "2019-03-20T22:17:09.329614Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(samples[:,:,2], bins=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-18T19:55:03.349607Z",
     "start_time": "2019-03-18T19:54:36.543030Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Learning Rate: 0.000010 ##########\n",
      "generator\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 50, 16)            1408      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 50, 16)            0         \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 50, 16)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 50, 2)             34        \n",
      "=================================================================\n",
      "Total params: 1,442\n",
      "Trainable params: 1,442\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "discriminator\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_2 (LSTM)                (None, 16)                1216      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 1,233\n",
      "Trainable params: 1,233\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "GAN\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 50, 5)             0         \n",
      "_________________________________________________________________\n",
      "sequential_1 (Sequential)    (None, 50, 2)             1442      \n",
      "_________________________________________________________________\n",
      "sequential_2 (Sequential)    (None, 1)                 1233      \n",
      "=================================================================\n",
      "Total params: 2,675\n",
      "Trainable params: 1,442\n",
      "Non-trainable params: 1,233\n",
      "_________________________________________________________________\n",
      "None\n",
      "--------------- Epoch 1 ---------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13d571a56639404ab6d3475fe552a19e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='# batch', max=150, style=ProgressStyle(description_width='ini…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-7d48f67e40b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0my_dis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mreal_lc_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgen_lc_y\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdiscriminator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m             \u001b[0md_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_dis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0;31m# Train generator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/keras/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/keras/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/keras/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/keras/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for l_rate in [.00001, .00002, .00005, .0001, .0002, .0005]:\n",
    "    # Build our GAN netowrk\n",
    "    print('########## Learning Rate: %f ##########' % (l_rate))\n",
    "    n_units = 16\n",
    "    adam = get_optimizer(l_rate)\n",
    "    generator = get_generator(adam, n_units, latent_dim,\n",
    "                              n_obs, n_feat, dropout=G_dropout)\n",
    "    discriminator = get_discriminator(adam, n_units, n_obs,\n",
    "                                      n_feat, dropout=D_dropout, \n",
    "                                      D_output=D_output)\n",
    "    print('generator')\n",
    "    print(generator.summary())\n",
    "    print('discriminator')\n",
    "    print(discriminator.summary())\n",
    "    gan = get_gan_network(discriminator, latent_dim, gen_lc_len, generator, adam)\n",
    "    print('GAN')\n",
    "    print(gan.summary())\n",
    "\n",
    "    test_noise = get_noise(n_examples, latent_dim=latent_dim)\n",
    "\n",
    "    # logger = TensorBoard(log_dir='%s/logs/%s_%s/run_%s/' % \n",
    "    #                      (main_path, model_name, data_name,\n",
    "    #                       datetime.datetime.now().strftime(\"%y%m%d_%I%M\")),\n",
    "    #                      histogram_freq=0, write_graph=False, write_images=True)\n",
    "    # logger.set_model(gan)\n",
    "    loss_G, loss_D = [], []\n",
    "\n",
    "    for e in range(1, n_epochs+1):\n",
    "        print('-'*15, 'Epoch %d' % e, '-'*15)\n",
    "        for n_batch in tnrange(batch_count, desc='# batch'):\n",
    "\n",
    "            # Get a random set of input noise and real LC's\n",
    "            real_lc = x_train[n_batch*batch_size: (n_batch+1)*batch_size]\n",
    "            # real_lc_y = np.random.uniform(.8, 1., size=batch_size)\n",
    "            real_lc_y = np.ones(batch_size) # * .9\n",
    "\n",
    "            # Generate fake LC's\n",
    "            noise = get_noise(batch_size, latent_dim=latent_dim)\n",
    "            gen_lc = generator.predict(noise)\n",
    "            # gen_lc_y = np.random.uniform(0., .2, size=batch_size)\n",
    "            gen_lc_y = np.ones(batch_size) * 0.\n",
    "\n",
    "            # Train discriminator on real\n",
    "            # discriminator.trainable = True\n",
    "            # d_loss_real, d_acc_real = discriminator.train_on_batch(real_lc, real_lc_y)\n",
    "            # Train discriminator on generated\n",
    "            # d_loss_gen, d_acc_gen = discriminator.train_on_batch(gen_lc, gen_lc_y)\n",
    "            # d_loss = d_loss_real + d_loss_gen\n",
    "            # d_acc = (d_acc_real + d_acc_gen)/2\n",
    "            \n",
    "            X = np.concatenate([real_lc, gen_lc])\n",
    "            y_dis = np.concatenate([real_lc_y, gen_lc_y])\n",
    "            discriminator.trainable = True\n",
    "            d_loss, d_acc = discriminator.train_on_batch(X, y_dis)\n",
    "\n",
    "            # Train generator\n",
    "            noise = get_noise(batch_size, latent_dim=latent_dim)\n",
    "            # y_gen = np.random.uniform(0.8, 1., size=batch_size)\n",
    "            y_gen = np.ones(batch_size) # * .9\n",
    "            discriminator.trainable = False\n",
    "            g_loss = gan.train_on_batch(noise, y_gen)\n",
    "            \n",
    "            loss_G.append([(e-1) * batch_count + n_batch ,g_loss])\n",
    "            loss_D.append([(e-1) * batch_count + n_batch ,d_loss])\n",
    "\n",
    "            # if n_batch % 10 == 0:\n",
    "            #     write_log_scalar(logger, ['G_loss', 'D_loss', 'D_acc'],\n",
    "            #                      [g_loss, d_loss, d_acc],\n",
    "            #                      (e-1) * batch_count + n_batch)\n",
    "\n",
    "        print(\"%d [D loss: %f, acc: %.0f%%] [G loss: %f]\" %\n",
    "              (e, d_loss, d_acc*100, g_loss))\n",
    "        if d_loss == 0:\n",
    "            print('Failiure mode...')\n",
    "            break\n",
    "\n",
    "        if e == 1 or e % viz_interval == 0:\n",
    "            lc_test = generator.predict(get_noise(n_examples, latent_dim=latent_dim))\n",
    "            img_buf = plot_generated_time_series(e, lc_test,\n",
    "                                                 test_lc=x_test[np.random.randint(0,len(x_test),8)])\n",
    "\n",
    "            # generator.save('%s/experiments/params/%s_%s_G_%i.hdf5' %\n",
    "            #               (main_path, model_name, data_name, e))\n",
    "            # discriminator.save('%s/experiments/params/%s_%s_D_%i.hdf5' %\n",
    "            #                   (main_path, model_name, data_name, e))\n",
    "\n",
    "            # write_log_plot(logger, img_buf, (e-1) * batch_count + n_batch)\n",
    "    \n",
    "    loss_G = np.array(loss_G)\n",
    "    loss_D = np.array(loss_D)\n",
    "    plt.plot(loss_G[:,0], loss_G[:,1], '-b', label='G', linewidth=1)\n",
    "    plt.plot(loss_D[:,0], loss_D[:,1], '-r', label='D', linewidth=1)\n",
    "    plt.xlabel('batch iter')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:keras]",
   "language": "python",
   "name": "conda-env-keras-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "581px",
    "left": "892px",
    "right": "20px",
    "top": "120px",
    "width": "368px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
