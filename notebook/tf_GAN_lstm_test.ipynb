{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-20T21:19:07.876656Z",
     "start_time": "2019-03-20T21:19:01.492939Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import io\n",
    "import datetime\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm_notebook, tnrange\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn import model_selection\n",
    "\n",
    "sys.path.append('%s/lib' % (os.path.abspath('..')))\n",
    "from SyntheticLCGenerator import synthetic_light_curve_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-20T21:19:07.913539Z",
     "start_time": "2019-03-20T21:19:07.884915Z"
    }
   },
   "outputs": [],
   "source": [
    "main_path = os.path.abspath('..')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-20T21:22:32.761079Z",
     "start_time": "2019-03-20T21:22:32.750518Z"
    }
   },
   "outputs": [],
   "source": [
    "# data\n",
    "num_samples = 214000\n",
    "seq_length = 50\n",
    "num_signals = 1\n",
    "use_time = False\n",
    "use_err = False\n",
    "n_feat = 3\n",
    "if (use_time and not use_err) or (not use_time and use_err):\n",
    "    n_feat = 2\n",
    "elif not use_time and not use_err:\n",
    "    n_feat = 1\n",
    "data_name = 'SynSine_time%s_err%s' % (str(use_time)[0], str(use_err)[0])\n",
    "\n",
    "# GAN architecture\n",
    "model_name = 'RGAN'\n",
    "learning_rate = 0.1\n",
    "latent_dim = 5\n",
    "D_hidden_units = 100\n",
    "G_hidden_units = 100\n",
    "D_output = 1\n",
    "dropout = 0.1\n",
    "batch_size = 28\n",
    "n_epochs = 200\n",
    "N_gen_feat = n_feat\n",
    "save_interval = 5\n",
    "viz_interval = 1\n",
    "batch_count = round(7000*.6 / batch_size)\n",
    "n_examples = 8\n",
    "gen_lc_len = n_obs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-20T21:21:09.926300Z",
     "start_time": "2019-03-20T21:21:09.882181Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_synthetic_time_series(data_path=None, n_samples=14000, seq_length=50,\n",
    "                               time_span=4, n_bands=1, n_signals=1, SNR_min=5,\n",
    "                               f0_interval='narrow',\n",
    "                               use_time=True, use_err=True):\n",
    "\n",
    "    if f0_interval == 'narrow':\n",
    "        f0_inter=[1/10., 1/1.]\n",
    "    elif f0_interval == 'wide':\n",
    "        f0_inter=[1/100., 1/0.01]\n",
    "    else:\n",
    "        print('Plese define frequency interval...')\n",
    "        return\n",
    "    if os.path.exists(data_path):\n",
    "        print('Loading from: ', data_path)\n",
    "        aux = np.load(data_path).item()\n",
    "        samples = aux['samples']\n",
    "        labels = np.array(aux['labels'])[:, None]\n",
    "        periods = aux['periods']\n",
    "        if n_bands == 1:\n",
    "            samples = samples.reshape(samples.shape[0], samples.shape[2],\n",
    "                                      samples.shape[3])\n",
    "        del aux\n",
    "        if use_time and not use_err:\n",
    "            samples = samples[:, :, 0:2]\n",
    "        if not use_time and not use_err:\n",
    "            samples = samples[:, :, 1:2]\n",
    "    else:\n",
    "        print('File does not exist...')\n",
    "    return samples, labels, periods\n",
    "\n",
    "\n",
    "def load_real_time_series(data_path=None, survey='EROS2', n_bands=2,\n",
    "                          use_time=True, use_err=True):\n",
    "    if os.path.exists(data_path):\n",
    "        print('Loading from: ', data_path)\n",
    "        aux = joblib.load(data_path)\n",
    "        lcs = aux['lcs']\n",
    "        meta = aux['meta']\n",
    "        del aux\n",
    "        lcs = np.stack([x.values for x in lcs])\n",
    "        print(lcs.shape)\n",
    "        if n_bands == 1:\n",
    "            lcs = lcs[:, :, 0:3]\n",
    "        if use_time and not use_err:\n",
    "            lcs = lcs[:, :, 0:2]\n",
    "        if not use_time and not use_err:\n",
    "            lcs = lcs[:, :, 1:2]\n",
    "    else:\n",
    "        print('No file...')\n",
    "        return\n",
    "    return lcs, meta\n",
    "\n",
    "\n",
    "def normalize(data, norm_time=False, scale_to=[0, 1], n_feat=2):\n",
    "    normed = np.zeros_like(data)\n",
    "    for i, lc in enumerate(data):     \n",
    "        for f in range(n_feat):\n",
    "            normed[i, :, f] = lc[:, f]\n",
    "            ## normalize time if asked\n",
    "            if f == 0 and norm_time:\n",
    "                normed[i, :, f] = (lc[:, f] - np.min(lc[:, f])) / \\\n",
    "                                  (np.max(lc[:, f]) - np.min(lc[:, f]))\n",
    "            ## normalize other feature values\n",
    "            if f > 0:\n",
    "                normed[i, :, f] = (lc[:, f] - np.min(lc[:, f])) / \\\n",
    "                                  (np.max(lc[:, f]) - np.min(lc[:, f]))\n",
    "            ## scale feature values if asked\n",
    "            if scale_to != [0, 1]:\n",
    "                normed[i, :, f] = (normed[i, :, f] * (scale_to[1] - scale_to[0])) + scale_to[0]\n",
    "    return normed\n",
    "\n",
    "\n",
    "def standarize(data, stand_time=False):\n",
    "    standar = np.zeros_like(data)\n",
    "    for i, lc in enumerate(data):\n",
    "        standar[i, :, 1] = (lc[:, 1] - np.mean(lc[:, 1])) / np.std(lc[:, 1])\n",
    "        if stand_time:\n",
    "            standar[i, :, 0] = (lc[:, 0] - np.mean(lc[:, 0])) / np.std(lc[:, 0])\n",
    "        else:\n",
    "            standar[i, :, 0] = lc[:, 0]\n",
    "    return standar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-20T21:22:36.923888Z",
     "start_time": "2019-03-20T21:22:36.876240Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File does not exist...\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'samples' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-492bc64ab0f1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m lightcurves, labels, periods = load_synthetic_time_series(data_path, \n\u001b[1;32m      4\u001b[0m                                                       \u001b[0muse_time\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_time\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m                                                       use_err=use_err)\n\u001b[0m\u001b[1;32m      6\u001b[0m lightcurves = normalize(lightcurves[:14000], norm_time=True, scale_to=[-1, 1], \n\u001b[1;32m      7\u001b[0m                     n_feat=num_generated_features)\n",
      "\u001b[0;32m<ipython-input-4-ddab7a5284df>\u001b[0m in \u001b[0;36mload_synthetic_time_series\u001b[0;34m(data_path, n_samples, seq_length, time_span, n_bands, n_signals, SNR_min, f0_interval, use_time, use_err)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'File does not exist...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperiods\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'samples' referenced before assignment"
     ]
    }
   ],
   "source": [
    "data_path = ('%s/data/synthetic/sine_nsamples%i_seqlength%i_nbands%i_nsig%i_timespan%i_SNR%i_f0%s.npy'\n",
    "             % (main_path, num_samples*2, seq_length, num_signals, 1, 4, 40, 'narrow'))\n",
    "lightcurves, labels, periods = load_synthetic_time_series(data_path, \n",
    "                                                      use_time=use_time,\n",
    "                                                      use_err=use_err)\n",
    "lightcurves = normalize(lightcurves[:14000], norm_time=True, scale_to=[-1, 1], \n",
    "                    n_feat=num_generated_features)\n",
    "labels = labels[:14000]\n",
    "periods = periods[:14000]\n",
    "print('Samples shape: ', lightcurves.shape)\n",
    "\n",
    "train, test, p_train, p_test = model_selection.train_test_split(lightcurves, labels,\n",
    "                                                                train_size=0.6)\n",
    "test, vali, p_test, p_vali = model_selection.train_test_split(test, p_test,\n",
    "                                                              train_size=0.5)\n",
    "lightcurves = {}\n",
    "lightcurves['train'] = train\n",
    "lightcurves['vali'] = vali\n",
    "lightcurves['test'] = test\n",
    "labels = {}\n",
    "labels['train'] = p_train\n",
    "labels['vali'] = p_vali\n",
    "labels['test'] = p_test\n",
    "\n",
    "del train, test, vali"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-23T00:36:18.035511Z",
     "start_time": "2019-02-23T00:36:18.030436Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_noise(n_samples, n_timesteps=50, latent_dim=5, use_time=True):\n",
    "    latent = np.random.normal(size=[n_samples, n_timesteps, latent_dim])\n",
    "    if use_time:\n",
    "        latent[:, :, 0] = np.linspace(0, 1.0, num=n_timesteps)\n",
    "    return latent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-23T01:44:40.576453Z",
     "start_time": "2019-02-23T01:44:40.568742Z"
    }
   },
   "outputs": [],
   "source": [
    "def generator(z, reuse=False):\n",
    "    with tf.variable_scope(\"generator\", reuse=tf.AUTO_REUSE):\n",
    "            \n",
    "        with tf.variable_scope(\"lstm1\"):\n",
    "            cell = tf.contrib.rnn.LSTMCell(num_units=G_hidden_units,\n",
    "                                           state_is_tuple=True, \n",
    "                                           initializer=None,\n",
    "                                           reuse=reuse)\n",
    "            rnn_output, rnn_states = tf.nn.dynamic_rnn(cell=cell, \n",
    "                                                       dtype=tf.float32,\n",
    "                                                       sequence_length=[n_obs]*batch_size,\n",
    "                                                       inputs=z)\n",
    "            print(rnn_output.shape)\n",
    "        with tf.variable_scope(\"reshape\"):\n",
    "            rnn_output_2d = tf.reshape(rnn_output, [-1, G_hidden_units])\n",
    "            print(rnn_output_2d.shape)\n",
    "\n",
    "        with tf.variable_scope(\"dense\"):\n",
    "            logits_2d = tf.layers.Dense(rnn_output_2d, 2, \n",
    "                                        activation_fn=tf.truncated_normal_initializer())\n",
    "            print(logits_2d.shape)\n",
    "            output_2d = tf.nn.tanh(logits_2d)\n",
    "\n",
    "        with tf.variable_scope(\"reshape\"):\n",
    "            out = tf.reshape(output_2d, [-1, n_obs, n_feat])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-23T01:44:40.987720Z",
     "start_time": "2019-02-23T01:44:40.978570Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def discriminator(x, reuse=False):\n",
    "    with tf.variable_scope(\"discriminator\", reuse=tf.AUTO_REUSE):\n",
    "\n",
    "        with tf.variable_scope(\"lstm1\"):\n",
    "            cell = tf.contrib.rnn.LSTMCell(num_units=D_hidden_units,\n",
    "                                           initializer=None,\n",
    "                                           state_is_tuple=True,\n",
    "                                           reuse=reuse)\n",
    "            rnn_output, rnn_states = tf.nn.dynamic_rnn(cell=cell, \n",
    "                                                       dtype=tf.float32,\n",
    "                                                       sequence_length=[n_obs]*batch_size,\n",
    "                                                       inputs=x)\n",
    "\n",
    "        with tf.variable_scope(\"dense\"):\n",
    "            logits = tf.contrib.layers.fully_connected(rnn_output, 1, activation_fn=None)\n",
    "\n",
    "        with tf.variable_scope(\"calss_prob\"):\n",
    "            \n",
    "            out = tf.nn.sigmoid(logits)\n",
    "            \n",
    "    return out, logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-23T01:44:41.873474Z",
     "start_time": "2019-02-23T01:44:41.863398Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"real_lc_9:0\", shape=(28, 50, 1), dtype=float32)\n",
      "Tensor(\"noise_9:0\", shape=(28, 50, 5), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "## Real Input\n",
    "X = tf.placeholder(tf.float32, shape=(batch_size, n_obs, n_feat), name='real_lc')\n",
    "print(X)\n",
    "## Latent Variables / Noise\n",
    "Z = tf.placeholder(tf.float32, shape=(batch_size, n_obs, latent_dim), name='noise')\n",
    "print(Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-23T01:44:42.758170Z",
     "start_time": "2019-02-23T01:44:42.577167Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 50, 100)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Trying to share variable generator/dense/fully_connected/weights, but specified shape (100, 2) and found shape (100, 100).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-e7b09d52e7a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Generator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mG_sample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mZ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'G_sample'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mG_sample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Discriminator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mD_real\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-58-878c9691bd92>\u001b[0m in \u001b[0;36mgenerator\u001b[0;34m(z, reuse)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dense\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             logits_2d = tf.contrib.layers.fully_connected(rnn_output, 2, \n\u001b[0;32m---> 19\u001b[0;31m                                                           activation_fn=None)\n\u001b[0m\u001b[1;32m     20\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits_2d\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0moutput_2d\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits_2d\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/keras/lib/python3.6/site-packages/tensorflow/contrib/framework/python/ops/arg_scope.py\u001b[0m in \u001b[0;36mfunc_with_args\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    180\u001b[0m       \u001b[0mcurrent_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurrent_scope\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey_func\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m       \u001b[0mcurrent_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcurrent_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m   \u001b[0m_add_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/keras/lib/python3.6/site-packages/tensorflow/contrib/layers/python/layers/layers.py\u001b[0m in \u001b[0;36mfully_connected\u001b[0;34m(inputs, num_outputs, activation_fn, normalizer_fn, normalizer_params, weights_initializer, weights_regularizer, biases_initializer, biases_regularizer, reuse, variables_collections, outputs_collections, trainable, scope)\u001b[0m\n\u001b[1;32m   1852\u001b[0m         \u001b[0m_scope\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1853\u001b[0m         _reuse=reuse)\n\u001b[0;32m-> 1854\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1855\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1856\u001b[0m     \u001b[0;31m# Add variables to collections.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/keras/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    815\u001b[0m       \u001b[0mOutput\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    816\u001b[0m     \"\"\"\n\u001b[0;32m--> 817\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    819\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_set_learning_phase_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/keras/lib/python3.6/site-packages/tensorflow/python/layers/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    372\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m       \u001b[0;31m# Actually call layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 374\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/keras/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    744\u001b[0m           \u001b[0;31m# the user has manually overwritten the build method do we need to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    745\u001b[0m           \u001b[0;31m# build it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 746\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    747\u001b[0m         \u001b[0;31m# We must set self.built since user defined build functions are not\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    748\u001b[0m         \u001b[0;31m# constrained to set self.built.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/keras/lib/python3.6/site-packages/tensorflow/python/keras/layers/core.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    942\u001b[0m         \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel_constraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    943\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 944\u001b[0;31m         trainable=True)\n\u001b[0m\u001b[1;32m    945\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_bias\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m       self.bias = self.add_weight(\n",
      "\u001b[0;32m~/miniconda3/envs/keras/lib/python3.6/site-packages/tensorflow/python/layers/base.py\u001b[0m in \u001b[0;36madd_weight\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, trainable, constraint, use_resource, synchronization, aggregation, partitioner)\u001b[0m\n\u001b[1;32m    286\u001b[0m             \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m             \u001b[0maggregation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maggregation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m             getter=vs.get_variable)\n\u001b[0m\u001b[1;32m    289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mregularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/keras/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36madd_weight\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, trainable, constraint, partitioner, use_resource, synchronization, aggregation, **kwargs)\u001b[0m\n\u001b[1;32m    607\u001b[0m         \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    608\u001b[0m         \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 609\u001b[0;31m         aggregation=aggregation)\n\u001b[0m\u001b[1;32m    610\u001b[0m     \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrack_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    611\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/keras/lib/python3.6/site-packages/tensorflow/python/training/checkpointable/base.py\u001b[0m in \u001b[0;36m_add_variable_with_custom_getter\u001b[0;34m(self, name, shape, dtype, initializer, getter, overwrite, **kwargs_for_getter)\u001b[0m\n\u001b[1;32m    637\u001b[0m     new_variable = getter(\n\u001b[1;32m    638\u001b[0m         \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 639\u001b[0;31m         **kwargs_for_getter)\n\u001b[0m\u001b[1;32m    640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m     \u001b[0;31m# If we set an initializer and the variable processed it, tracking will not\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/keras/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m   1485\u001b[0m       \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1486\u001b[0m       \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1487\u001b[0;31m       aggregation=aggregation)\n\u001b[0m\u001b[1;32m   1488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1489\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/keras/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, var_store, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m   1235\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1236\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1237\u001b[0;31m           aggregation=aggregation)\n\u001b[0m\u001b[1;32m   1238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1239\u001b[0m   def _get_partitioned_variable(self,\n",
      "\u001b[0;32m~/miniconda3/envs/keras/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m    521\u001b[0m           function_utils.has_kwargs(custom_getter)):\n\u001b[1;32m    522\u001b[0m         \u001b[0mcustom_getter_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"constraint\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconstraint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 523\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mcustom_getter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mcustom_getter_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    524\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m       return _true_getter(\n",
      "\u001b[0;32m~/miniconda3/envs/keras/lib/python3.6/site-packages/tensorflow/contrib/layers/python/layers/layers.py\u001b[0m in \u001b[0;36mlayer_variable_getter\u001b[0;34m(getter, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1747\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mlayer_variable_getter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1748\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'rename'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrename\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1749\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_model_variable_getter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1750\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1751\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mlayer_variable_getter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/keras/lib/python3.6/site-packages/tensorflow/contrib/layers/python/layers/layers.py\u001b[0m in \u001b[0;36m_model_variable_getter\u001b[0;34m(getter, name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, rename, use_resource, synchronization, aggregation, **_)\u001b[0m\n\u001b[1;32m   1738\u001b[0m       \u001b[0muse_resource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_resource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1739\u001b[0m       \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1740\u001b[0;31m       aggregation=aggregation)\n\u001b[0m\u001b[1;32m   1741\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/keras/lib/python3.6/site-packages/tensorflow/contrib/framework/python/ops/arg_scope.py\u001b[0m in \u001b[0;36mfunc_with_args\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    180\u001b[0m       \u001b[0mcurrent_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurrent_scope\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey_func\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m       \u001b[0mcurrent_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcurrent_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m   \u001b[0m_add_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/keras/lib/python3.6/site-packages/tensorflow/contrib/framework/python/ops/variables.py\u001b[0m in \u001b[0;36mmodel_variable\u001b[0;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, device, partitioner, custom_getter, use_resource, synchronization, aggregation)\u001b[0m\n\u001b[1;32m    348\u001b[0m       \u001b[0muse_resource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_resource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m       \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 350\u001b[0;31m       aggregation=aggregation)\n\u001b[0m\u001b[1;32m    351\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mvar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/keras/lib/python3.6/site-packages/tensorflow/contrib/framework/python/ops/arg_scope.py\u001b[0m in \u001b[0;36mfunc_with_args\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    180\u001b[0m       \u001b[0mcurrent_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurrent_scope\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey_func\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m       \u001b[0mcurrent_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcurrent_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m   \u001b[0m_add_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/keras/lib/python3.6/site-packages/tensorflow/contrib/framework/python/ops/variables.py\u001b[0m in \u001b[0;36mvariable\u001b[0;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, device, partitioner, custom_getter, use_resource, synchronization, aggregation)\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0muse_resource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_resource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m         aggregation=aggregation)\n\u001b[0m\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/keras/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_true_getter\u001b[0;34m(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m    490\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 492\u001b[0;31m           aggregation=aggregation)\n\u001b[0m\u001b[1;32m    493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m     \u001b[0;31m# Set trainable value based on synchronization value.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/keras/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_get_single_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape, use_resource, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m    864\u001b[0m         raise ValueError(\"Trying to share variable %s, but specified shape %s\"\n\u001b[1;32m    865\u001b[0m                          \" and found shape %s.\" % (name, shape,\n\u001b[0;32m--> 866\u001b[0;31m                                                    found_var.get_shape()))\n\u001b[0m\u001b[1;32m    867\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfound_var\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    868\u001b[0m         \u001b[0mdtype_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Trying to share variable generator/dense/fully_connected/weights, but specified shape (100, 2) and found shape (100, 100)."
     ]
    }
   ],
   "source": [
    "# Generator\n",
    "G_sample = generator(Z)\n",
    "print('G_sample', G_sample)\n",
    "# Discriminator\n",
    "D_real = discriminator(X)\n",
    "D_fake = discriminator(G_sample)\n",
    "print(D_real)\n",
    "print(D_fake)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-23T01:00:43.414753Z",
     "start_time": "2019-02-23T01:00:43.402674Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor 'ones_3:0' shape=(50, 1) dtype=float32>,\n",
       " <tf.Tensor 'ones_like_3:0' shape=(2, ?, 50, 1) dtype=float32>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.ones((50, 1)), tf.ones_like(D_real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-23T00:36:34.935865Z",
     "start_time": "2019-02-23T00:36:34.884697Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Generator\n",
    "G_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "    logits=D_fake, labels=tf.ones_like(D_fake)))\n",
    "\n",
    "# Discriminator\n",
    "D_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "    logits=D_real, labels=tf.ones_like(D_real)))\n",
    "\n",
    "D_loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "    logits=D_fake, labels=tf.zeros_like(D_fake)))\n",
    "\n",
    "D_loss = D_loss_real + D_loss_fake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-23T00:36:37.698688Z",
     "start_time": "2019-02-23T00:36:37.693147Z"
    }
   },
   "outputs": [],
   "source": [
    "# Obtain trainable variables for both networks\n",
    "train_vars = tf.trainable_variables()\n",
    "\n",
    "G_vars = [var for var in train_vars if 'generator' in var.name]\n",
    "D_vars = [var for var in train_vars if 'discriminator' in var.name]\n",
    "\n",
    "num_epochs = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-23T00:36:41.670801Z",
     "start_time": "2019-02-23T00:36:40.326568Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "G_opt = tf.train.AdamOptimizer(\n",
    "    learning_rate=learning_rate).minimize(G_loss, var_list=G_vars,)\n",
    "D_opt = tf.train.AdamOptimizer(\n",
    "    learning_rate=learning_rate).minimize(D_loss, var_list=D_vars,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helping functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-23T00:36:52.868878Z",
     "start_time": "2019-02-23T00:36:52.855234Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a wall of generated time series\n",
    "def plot_generated_time_series(epoch, generated_lc, \n",
    "                               test_lc=None, examples=8,\n",
    "                               dim=(2, 4), figsize=(16, 4),\n",
    "                               use_time=True, use_err=True):\n",
    "\n",
    "    fig, ax = plt.subplots(nrows=dim[0], ncols=dim[1], figsize=figsize)\n",
    "    fig.suptitle('Real (black), Gen (blue) LCs Epoch %i' % (epoch), \n",
    "                 fontsize=20, y=1.05)\n",
    "    for i in range(dim[0]):\n",
    "        for j in range(dim[1]):\n",
    "            if use_time and use_err:\n",
    "                if j == 0 and test_lc is not None:\n",
    "                    ax[i,j].errorbar(test_lc[i, :, 0], test_lc[i, :, 1],\n",
    "                                     yerr=test_lc[i, :, 2], fmt='k.')\n",
    "                else:\n",
    "                    ax[i,j].errorbar(generated_lc[(j+i) + j, :, 0], \n",
    "                                     generated_lc[(j+i) + j, :, 1],\n",
    "                                     yerr=generated_lc[(j+i) + j, :, 2], \n",
    "                                     fmt='.', c='royalblue')\n",
    "            elif use_time and not use_err:\n",
    "                if j == 0 and test_lc is not None:\n",
    "                    ax[i,j].plot(test_lc[i, :, 0], test_lc[i, :, 1], 'k.')\n",
    "                else:\n",
    "                    ax[i,j].plot(generated_lc[(j+i) + j, :, 0], \n",
    "                                 generated_lc[(j+i) + j, :, 1], '.', \n",
    "                                 c='royalblue')\n",
    "            elif not use_time and not use_err:\n",
    "                if j == 0 and test_lc is not None:\n",
    "                    ax[i,j].plot(test_lc[i, :], 'k.')\n",
    "                else:\n",
    "                    ax[i,j].plot(generated_lc[(j+i) + j, :], '.', \n",
    "                                 c='royalblue')\n",
    "    plt.tight_layout()\n",
    "    # plt.savefig('gan_generated_image_epoch_%d.png' % epoch)\n",
    "    # buf = io.BytesIO()\n",
    "    # plt.savefig(buf, format='png')\n",
    "    # buf.seek(0)\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "def write_log_scalar(callback, names, logs, step):\n",
    "    for name, value in zip(names, logs):\n",
    "        summary = tf.Summary()\n",
    "        summary_value = summary.value.add()\n",
    "        summary_value.simple_value = value\n",
    "        summary_value.tag = name\n",
    "        callback.writer.add_summary(summary, step)\n",
    "        callback.writer.flush()\n",
    "        \n",
    "def write_log_plot(callback, buf, step):\n",
    "    image = tf.image.decode_png(buf.getvalue(), channels=4)\n",
    "    image = tf.expand_dims(image, 0)\n",
    "    summary = tf.summary.image(\"Generated_Image\", image,\n",
    "                               max_outputs=1)\n",
    "    callback.writer.add_summary(tf.keras.backend.eval(summary), step)\n",
    "    callback.writer.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-23T00:36:59.515197Z",
     "start_time": "2019-02-23T00:36:59.510630Z"
    }
   },
   "outputs": [],
   "source": [
    "test_noise = get_noise(n_examples, n_timesteps=n_obs, \n",
    "                       latent_dim=latent_dim, use_time=use_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-23T00:41:55.508436Z",
     "start_time": "2019-02-23T00:41:55.208167Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Epoch 0/200 --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jorgetil/miniconda3/envs/keras/lib/python3.6/site-packages/tensorflow/python/client/session.py:1702: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed0633f87bf442c1846155f795c1d4e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='# batch', max=150, style=ProgressStyle(description_width='ini…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "assertion failed: [Expected shape for Tensor generator/lstm1/rnn/sequence_length:0 is ] [8] [ but saw shape: ] [28]\n\t [[node generator/lstm1/rnn/Assert/Assert (defined at <ipython-input-7-43be46e4a520>:12)  = Assert[T=[DT_STRING, DT_INT32, DT_STRING, DT_INT32], summarize=3, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](generator/lstm1/rnn/All, generator/lstm1/rnn/Assert/Assert/data_0, generator/lstm1/rnn/stack, generator/lstm1/rnn/Assert/Assert/data_2, generator/lstm1/rnn/Shape_1)]]\n\nCaused by op 'generator/lstm1/rnn/Assert/Assert', defined at:\n  File \"/Users/jorgetil/miniconda3/envs/keras/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/Users/jorgetil/miniconda3/envs/keras/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/Users/jorgetil/miniconda3/envs/keras/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/Users/jorgetil/miniconda3/envs/keras/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/Users/jorgetil/miniconda3/envs/keras/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 505, in start\n    self.io_loop.start()\n  File \"/Users/jorgetil/miniconda3/envs/keras/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 132, in start\n    self.asyncio_loop.run_forever()\n  File \"/Users/jorgetil/miniconda3/envs/keras/lib/python3.6/asyncio/base_events.py\", line 438, in run_forever\n    self._run_once()\n  File \"/Users/jorgetil/miniconda3/envs/keras/lib/python3.6/asyncio/base_events.py\", line 1451, in _run_once\n    handle._run()\n  File \"/Users/jorgetil/miniconda3/envs/keras/lib/python3.6/asyncio/events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"/Users/jorgetil/miniconda3/envs/keras/lib/python3.6/site-packages/tornado/ioloop.py\", line 758, in _run_callback\n    ret = callback()\n  File \"/Users/jorgetil/miniconda3/envs/keras/lib/python3.6/site-packages/tornado/stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/jorgetil/miniconda3/envs/keras/lib/python3.6/site-packages/tornado/gen.py\", line 1233, in inner\n    self.run()\n  File \"/Users/jorgetil/miniconda3/envs/keras/lib/python3.6/site-packages/tornado/gen.py\", line 1147, in run\n    yielded = self.gen.send(value)\n  File \"/Users/jorgetil/miniconda3/envs/keras/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 357, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"/Users/jorgetil/miniconda3/envs/keras/lib/python3.6/site-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/Users/jorgetil/miniconda3/envs/keras/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 267, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"/Users/jorgetil/miniconda3/envs/keras/lib/python3.6/site-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/Users/jorgetil/miniconda3/envs/keras/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 534, in execute_request\n    user_expressions, allow_stdin,\n  File \"/Users/jorgetil/miniconda3/envs/keras/lib/python3.6/site-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/Users/jorgetil/miniconda3/envs/keras/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 294, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/Users/jorgetil/miniconda3/envs/keras/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/Users/jorgetil/miniconda3/envs/keras/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2819, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/Users/jorgetil/miniconda3/envs/keras/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2845, in _run_cell\n    return runner(coro)\n  File \"/Users/jorgetil/miniconda3/envs/keras/lib/python3.6/site-packages/IPython/core/async_helpers.py\", line 67, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/Users/jorgetil/miniconda3/envs/keras/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3020, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/Users/jorgetil/miniconda3/envs/keras/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3185, in run_ast_nodes\n    if (yield from self.run_code(code, result)):\n  File \"/Users/jorgetil/miniconda3/envs/keras/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3267, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-10-f77800090980>\", line 2, in <module>\n    G_sample = generator(Z)\n  File \"<ipython-input-7-43be46e4a520>\", line 12, in generator\n    inputs=z)\n  File \"/Users/jorgetil/miniconda3/envs/keras/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py\", line 651, in dynamic_rnn\n    [_assert_has_shape(sequence_length, [batch_size])]):\n  File \"/Users/jorgetil/miniconda3/envs/keras/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py\", line 646, in _assert_has_shape\n    packed_shape, \" but saw shape: \", x_shape])\n  File \"/Users/jorgetil/miniconda3/envs/keras/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py\", line 189, in wrapped\n    return _add_should_use_warning(fn(*args, **kwargs))\n  File \"/Users/jorgetil/miniconda3/envs/keras/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 159, in Assert\n    return gen_logging_ops._assert(condition, data, summarize, name=\"Assert\")\n  File \"/Users/jorgetil/miniconda3/envs/keras/lib/python3.6/site-packages/tensorflow/python/ops/gen_logging_ops.py\", line 52, in _assert\n    name=name)\n  File \"/Users/jorgetil/miniconda3/envs/keras/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/Users/jorgetil/miniconda3/envs/keras/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 488, in new_func\n    return func(*args, **kwargs)\n  File \"/Users/jorgetil/miniconda3/envs/keras/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3274, in create_op\n    op_def=op_def)\n  File \"/Users/jorgetil/miniconda3/envs/keras/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1770, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nInvalidArgumentError (see above for traceback): assertion failed: [Expected shape for Tensor generator/lstm1/rnn/sequence_length:0 is ] [8] [ but saw shape: ] [28]\n\t [[node generator/lstm1/rnn/Assert/Assert (defined at <ipython-input-7-43be46e4a520>:12)  = Assert[T=[DT_STRING, DT_INT32, DT_STRING, DT_INT32], summarize=3, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](generator/lstm1/rnn/All, generator/lstm1/rnn/Assert/Assert/data_0, generator/lstm1/rnn/stack, generator/lstm1/rnn/Assert/Assert/data_2, generator/lstm1/rnn/Shape_1)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m~/miniconda3/envs/keras/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/keras/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/keras/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: assertion failed: [Expected shape for Tensor generator/lstm1/rnn/sequence_length:0 is ] [8] [ but saw shape: ] [28]\n\t [[{{node generator/lstm1/rnn/Assert/Assert}} = Assert[T=[DT_STRING, DT_INT32, DT_STRING, DT_INT32], summarize=3, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](generator/lstm1/rnn/All, generator/lstm1/rnn/Assert/Assert/data_0, generator/lstm1/rnn/stack, generator/lstm1/rnn/Assert/Assert/data_2, generator/lstm1/rnn/Shape_1)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-1fd90ca2d1c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m         _, d_error, d_pred_real, d_pred_fake = session.run([D_opt, D_loss,\n\u001b[1;32m     17\u001b[0m                                                             D_real, D_fake],\n\u001b[0;32m---> 18\u001b[0;31m                                                            feed_dict=feed_dict)\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m# 2. Train Generator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/keras/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/keras/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/keras/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/keras/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1346\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1347\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror_interpolation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1348\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1350\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: assertion failed: [Expected shape for Tensor generator/lstm1/rnn/sequence_length:0 is ] [8] [ but saw shape: ] [28]\n\t [[node generator/lstm1/rnn/Assert/Assert (defined at <ipython-input-7-43be46e4a520>:12)  = Assert[T=[DT_STRING, DT_INT32, DT_STRING, DT_INT32], summarize=3, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](generator/lstm1/rnn/All, generator/lstm1/rnn/Assert/Assert/data_0, generator/lstm1/rnn/stack, generator/lstm1/rnn/Assert/Assert/data_2, generator/lstm1/rnn/Shape_1)]]\n\nCaused by op 'generator/lstm1/rnn/Assert/Assert', defined at:\n  File \"/Users/jorgetil/miniconda3/envs/keras/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/Users/jorgetil/miniconda3/envs/keras/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/Users/jorgetil/miniconda3/envs/keras/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/Users/jorgetil/miniconda3/envs/keras/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/Users/jorgetil/miniconda3/envs/keras/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 505, in start\n    self.io_loop.start()\n  File \"/Users/jorgetil/miniconda3/envs/keras/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 132, in start\n    self.asyncio_loop.run_forever()\n  File \"/Users/jorgetil/miniconda3/envs/keras/lib/python3.6/asyncio/base_events.py\", line 438, in run_forever\n    self._run_once()\n  File \"/Users/jorgetil/miniconda3/envs/keras/lib/python3.6/asyncio/base_events.py\", line 1451, in _run_once\n    handle._run()\n  File \"/Users/jorgetil/miniconda3/envs/keras/lib/python3.6/asyncio/events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"/Users/jorgetil/miniconda3/envs/keras/lib/python3.6/site-packages/tornado/ioloop.py\", line 758, in _run_callback\n    ret = callback()\n  File \"/Users/jorgetil/miniconda3/envs/keras/lib/python3.6/site-packages/tornado/stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/jorgetil/miniconda3/envs/keras/lib/python3.6/site-packages/tornado/gen.py\", line 1233, in inner\n    self.run()\n  File \"/Users/jorgetil/miniconda3/envs/keras/lib/python3.6/site-packages/tornado/gen.py\", line 1147, in run\n    yielded = self.gen.send(value)\n  File \"/Users/jorgetil/miniconda3/envs/keras/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 357, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"/Users/jorgetil/miniconda3/envs/keras/lib/python3.6/site-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/Users/jorgetil/miniconda3/envs/keras/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 267, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"/Users/jorgetil/miniconda3/envs/keras/lib/python3.6/site-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/Users/jorgetil/miniconda3/envs/keras/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 534, in execute_request\n    user_expressions, allow_stdin,\n  File \"/Users/jorgetil/miniconda3/envs/keras/lib/python3.6/site-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/Users/jorgetil/miniconda3/envs/keras/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 294, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/Users/jorgetil/miniconda3/envs/keras/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/Users/jorgetil/miniconda3/envs/keras/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2819, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/Users/jorgetil/miniconda3/envs/keras/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2845, in _run_cell\n    return runner(coro)\n  File \"/Users/jorgetil/miniconda3/envs/keras/lib/python3.6/site-packages/IPython/core/async_helpers.py\", line 67, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/Users/jorgetil/miniconda3/envs/keras/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3020, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/Users/jorgetil/miniconda3/envs/keras/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3185, in run_ast_nodes\n    if (yield from self.run_code(code, result)):\n  File \"/Users/jorgetil/miniconda3/envs/keras/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3267, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-10-f77800090980>\", line 2, in <module>\n    G_sample = generator(Z)\n  File \"<ipython-input-7-43be46e4a520>\", line 12, in generator\n    inputs=z)\n  File \"/Users/jorgetil/miniconda3/envs/keras/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py\", line 651, in dynamic_rnn\n    [_assert_has_shape(sequence_length, [batch_size])]):\n  File \"/Users/jorgetil/miniconda3/envs/keras/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py\", line 646, in _assert_has_shape\n    packed_shape, \" but saw shape: \", x_shape])\n  File \"/Users/jorgetil/miniconda3/envs/keras/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py\", line 189, in wrapped\n    return _add_should_use_warning(fn(*args, **kwargs))\n  File \"/Users/jorgetil/miniconda3/envs/keras/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 159, in Assert\n    return gen_logging_ops._assert(condition, data, summarize, name=\"Assert\")\n  File \"/Users/jorgetil/miniconda3/envs/keras/lib/python3.6/site-packages/tensorflow/python/ops/gen_logging_ops.py\", line 52, in _assert\n    name=name)\n  File \"/Users/jorgetil/miniconda3/envs/keras/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/Users/jorgetil/miniconda3/envs/keras/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 488, in new_func\n    return func(*args, **kwargs)\n  File \"/Users/jorgetil/miniconda3/envs/keras/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3274, in create_op\n    op_def=op_def)\n  File \"/Users/jorgetil/miniconda3/envs/keras/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1770, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nInvalidArgumentError (see above for traceback): assertion failed: [Expected shape for Tensor generator/lstm1/rnn/sequence_length:0 is ] [8] [ but saw shape: ] [28]\n\t [[node generator/lstm1/rnn/Assert/Assert (defined at <ipython-input-7-43be46e4a520>:12)  = Assert[T=[DT_STRING, DT_INT32, DT_STRING, DT_INT32], summarize=3, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](generator/lstm1/rnn/All, generator/lstm1/rnn/Assert/Assert/data_0, generator/lstm1/rnn/stack, generator/lstm1/rnn/Assert/Assert/data_2, generator/lstm1/rnn/Shape_1)]]\n"
     ]
    }
   ],
   "source": [
    "# Start interactive session\n",
    "session = tf.InteractiveSession()\n",
    "# Init Variables\n",
    "tf.global_variables_initializer().run()\n",
    "\n",
    "# Iterate through epochs\n",
    "for epoch in range(n_epochs):\n",
    "    print('-------- Epoch %i/%i --------' % (epoch, n_epochs))\n",
    "    for n_batch in tnrange(batch_count, desc='# batch'):\n",
    "\n",
    "        # 1. Train Discriminator\n",
    "        X_batch = x_train[n_batch * batch_size: (n_batch + 1) * batch_size]\n",
    "        Z_batch = get_noise(n_examples, n_timesteps=n_obs,\n",
    "                            latent_dim=latent_dim, use_time=use_time)\n",
    "        feed_dict = {X: X_batch, Z: Z_batch}\n",
    "        _, d_error, d_pred_real, d_pred_fake = session.run([D_opt, D_loss,\n",
    "                                                            D_real, D_fake],\n",
    "                                                           feed_dict=feed_dict)\n",
    "\n",
    "        # 2. Train Generator\n",
    "        Z_batch_g = get_noise(n_examples, n_timesteps=n_obs,\n",
    "                              latent_dim=latent_dim, use_time=use_time)\n",
    "        feed_dict = {Z: Z_batch_g}\n",
    "        _, g_error = session.run([G_opt, G_loss],\n",
    "                                 feed_dict=feed_dict)\n",
    "\n",
    "        if n_batch % 100 == 0:\n",
    "            display.clear_output(True)\n",
    "            # Generate images from test noise\n",
    "            test_lc = session.run(G_sample, feed_dict={Z: test_noise})\n",
    "            _ = plot_generated_time_series(e, test_lc,\n",
    "                                           test_lc=x_test[np.random.randint(0,len(x_test),2)])\n",
    "session.InteractiveSession.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:keras]",
   "language": "python",
   "name": "conda-env-keras-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
